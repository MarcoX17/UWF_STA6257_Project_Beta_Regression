---
title: "Predicting Students' Chance of Admission Using Beta Regression"
author: "Marcus Chery and Keiron Green"
date: '`r Sys.Date()`'

format: 
  html:
    code-fold: true
course: STA 6257 - Advanced Statistical Modeling
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

## Introduction

Beta regression is a statistical methodology tailored to address the
challenges associated with modeling continuous bounded response
variables, often constrained within the open interval (0, 1). Beta
regression, as introduced by [@ferrari2004] in their seminal paper,
offers an elegant framework for modeling proportions and percentages
with an emphasis on interpretability of model parameters. This
foundational work serves as a cornerstone for subsequent research and
applications.

This report delves into the application of beta regression within the
context of heart disease data analysis. Our objective is to provide a
thorough understanding of beta regression, its practical implementation
using the "betareg" package in R, and its real-world relevance through
heart disease data. Throughout the report, we offer practical examples,
code snippets, and graphs to facilitate hands-on learning. Additionally,
we address the limitations of beta regression and discuss potential
future research directions. The goal of this report is to equip readers
with the knowledge and skills needed to effectively utilize beta
regression in heart disease analysis, while also fostering awareness of
its boundaries and avenues for further exploration.

In beta regression, the dependent variable is assumed to follow a beta
distribution, and the goal is to model its relationship with one or more
independent variables. Beta regression is suitable for data where the
response variable lies within a closed interval, such as proportions,
percentages, or rates. It considers both the mean and the dispersion of
the data [@ferrari2004] .

The beta distribution is characterized by two parameters: alpha (α) and
beta (β). These parameters can be modeled as a function of predictor
variables, allowing for the analysis of how independent variables
influence the shape of the distribution.

Like logistic regression, beta regression often uses a link function to
relate the mean of the beta distribution to the linear combination of
predictor variables. Common link functions include logit, probit, and
log-log. The coefficients obtained from beta regression provide insights
into how changes in the predictor variables affect the distribution of
the response variable, including its mean and variability.

Complementing this, the "betareg" package for R, as presented by
[@cri2010], provides various methods and procedures for implementing
beta regression models in R. Building on the foundation laid by
[@ferrari2004], [@cri2010] introduce the "betareg" package for R,
offering a practical implementation of beta regression models. This
paper is instrumental in bringing beta regression into the realm of data
analysis and showcases the package's utility in addressing challenges
related to continuous bounded response variables.

[@guolo2014] seek to expand on beta regression. The authors incorporate
a Gaussian copula to model serial dependence in the data, making it
applicable to time series analysis. The study's practical application
involves analyzing influenza-like-illness incidence data from the Google
Flu Trends project. Their methodology provides a framework for analyzing
bounded time series data and offers insights into inference, prediction,
and control in this context.

Moreover, the paper authored by [@espinheira2008] unveils innovative
residuals, based on Fisher's scoring iterative algorithm, enhancing
model assessments and bias correction---a pivotal contribution to the
methodology's practical applicability. They address a common challenge
in statistical analysis by offering a practical solution for improved
model assessments and bias correction. The study presents new residuals
based on Fisher's scoring iterative algorithm, demonstrating their
superiority in various scenarios, particularly in small sample sizes.

Various papers throughout the years have sought to improve the basic
beta regression techniques of [@ferrari2004]. [@simas2010] delve into
bias correction methods for beta regression models in the quest for
improved estimators in beta regression. They address biases in parameter
estimation, a critical concern. Recognizing the potential for bias in
parameter estimation, especially in small sample sizes, the paper
provides closed-form expressions for second-order biases in maximum
likelihood estimators. The authors present various methods for bias
correction, including analytical approaches and bootstrap bias
adjustment, enhancing the accuracy of model assessments and mitigating
erroneous conclusions.

Likewise, [@schmid2013] introduced one such enhancement known as the
"boosted beta regression" as a novel extension of classical beta
regression. [@schmid2013] address complex modeling scenarios with many
predictor variables, multicollinearity, nonlinear relationships, and
overdispersion dependencies. They utilize the gamboostLSS algorithm to
efficiently estimate beta regression models, making it possible to fit
models while preserving the structure of beta regression. The method's
effectiveness is demonstrated through ecological data applications,
highlighting its advantages in terms of model fit and prediction
accuracy.

[@abonazel2022] took a different approach. They focused on improving the
estimation of beta regression models when multicollinearity was present
among regressors. They introduced the Dawoud--Kibria estimator as an
alternative to the conventional maximum likelihood estimator (BML) when
dealing with such situations. The authors derived the properties of this
new estimator and assess its performance through theoretical
comparisons, Monte Carlo simulations, and a real-life application.

Finally, it is important to note the challenges of beta regression.
[@douma2019] addresses one challenge of analyzing proportional data in
ecology and evolution. Proportional data, common in these fields, pose
unique statistical challenges due to their bounded nature and varying
variances. The paper introduces beta and Dirichlet regression techniques
as effective tools for analyzing such data. These methods allow for
analysis at the original scale, reducing bias and simplifying
interpretation. The article provides practical guidance on model fitting
and reporting.

[@couri2022] focuses on the computational challenges of estimating
parameters in beta regression models. The authors evaluate ten
algorithms for parameter estimation and find that four, including
simulated annealing, perform well. The paper also examines the optim
function in R and suggests its use in challenging cases. Simulated
annealing is recommended, particularly when the optim function fails.
Overall, the research offers insights into improving parameter
estimation for beta regression models in various applications.

Furthermore, one such limitation of classical beta regression is its
handling of zero or one inflated data. [@ospina2012] introduce a
versatile class of regression models, zero-or-one inflated beta
regression, specifically tailored to address datasets containing
continuous proportions with zeros or ones. The paper offers a flexible
solution for modeling mixed continuous-discrete distributions, essential
in real-world scenarios such as income contributions or work-related
activities. By combining the beta distribution with a degenerate
distribution at zero or one, the authors provide a robust framework for
effective modeling, parameter estimation, diagnostics, and model
selection.

## Methods

Our objective is to investigate the relationship and effect of the given
variables on the response variable which in this case is the cholesterol
count proportional. Being that the chosen response is proportional in
nature, linear regression is not appropriate. The suitable approach is
the implementation of beta regression. The reason as to why linear
regression is not appropriate is due predicted values falling outside
the bound of (0,1) where as all the proportional values are within the
bounds of 0 and 1.Our objective is to implement beta regression to
predict the proportion values based on grouped cholesterol values.In
other words,using beta regression to predict the proportion of a
specific cholesterol value given that chosen cholesterol value.Before
implement the beta regression, it was necessary to create the
proportional variable. This was done by counting similar cholesterol
values and grouping them together then dividing the grouped values by
the total number of examples in the data set.

After the creation the proportional response,
Cholesterol_count_proportional,the beta regression algorithm is
implemented.Before delving into the implementation of the beta
regression let's briefly analyze the mathematics of the beta regression
algorithm. The beta regression is based on the logit function. The logit
function plots probabilities between the interval 0 and 1. The logit
function can be seen below. $$ logit(p) = log(p/(1-p))$$

Another formula which will need to be accounted for in predicting the
proportionality values is the beta density.The beta density is
especially useful in modelling probabilities as it works on the interval
\[0,1\] When implementing beta regression it is assumed that the
response variable follows a beta distribution hence the name beta
regression.

The beta density formula is below. $$ 
f(x;\alpha,\beta) =\frac{ \Gamma(p + q)}{\Gamma(p)*\Gamma(q)}x^{p-1}(1-x)^{q-1}
$$ , $$ 0 < y < 1 $$ where $p,q$ \> 0 and $\Gamma (.)$ is the Gamma
function. An alternate parameterization was proposed by (Ferrari and
Cribari-Neto 2004) by setting $u = p/(p+q)$ and $\phi = p + q$

$$
 f(y;\mu,\phi)=\frac{\Gamma(\phi)}{\Gamma(\mu\phi)\Gamma((1-\mu)\phi)} y^{\mu\phi-1}(1-y)^{(1-\mu)(\phi-1)}
$$

$$0<y<1$$

with $0 < u < 1$ and $\phi > 0$.

We denote $y\sim B(u,\phi)$ where $E(y)=u$ and $VAR(y)=u(1-u)/(1+\phi)$.
The $\phi$ is known as the precision parameter. For a fixed value of $u$
as the $\phi$ increases the variance of the response variable $y$
decreases.

Let $y_1,.....,y_n$ be a random sample set given that
\$y_i\simB(u_i,\phi), i = 1,......,n. The beta regression model is
defined as $$ g(u_i) = x^T_iB = n_i$$ where $B=(B_1,....,B_k)$ is a
$k*1$ vector of unknown regression parameters
$(k<n),x_i=(x_i,......x_n)^T$ is the vector of k regressors(independent
variables or covariates) and $n_i$ is a linear predictor -
i.e.,$n_i = B_1x_{i1}+...+B_kx_{ik};$ usually $x_{i1}$ = 1 for all $i$
so that the model has an intercept.The $g(.): (0,1) -> IR$ represent the
link function.

## Analysis and Results

This section aims to identify key variables for constructing a beta
regression model to predict admission chances. Initially, we analyze the
dataset to determine the correlation between various predictors and the
chance of admission. Following this, we use these insights to develop
and evaluate several models, focusing on their predictive accuracy and
effectiveness. This process is critical for establishing a reliable
model that accurately forecasts admission probabilities based on the
identified significant predictors.

### Data and Visualization

Data: [**University Admission
Data**](https://www.kaggle.com/datasets/akshaydattatraykhare/data-for-admission-in-the-university/data)

### Attribute Information

+--------------+--------------+--------------+--------------+
| V a r i a    | P a r a m e  | R a nge      | D e s c r i  |
| ble          | ter          |              | p t ion      |
+==============+==============+:=============+==============+
| GRE S c o    | g r e \_ s c | 2 9 0 - 340  | Q u a n t i  |
| res          | ore          |              | f ies a c a  |
|              |              | ( 340 s c a  | n d i d a t  |
|              |              | le)          | e's p e r f  |
|              |              |              | o r m a nce  |
|              |              |              | on the G r a |
|              |              |              | d u ate R e  |
|              |              |              | c ord E x a  |
|              |              |              | m i n a t i  |
|              |              |              | on, w ith a  |
|              |              |              | m a x i mum  |
|              |              |              | s c ore of   |
|              |              |              | 340          |
+--------------+--------------+--------------+--------------+
| T O EFL S c  | t o e f l \_ | 9 2 - 120    | M e a s u    |
| o res        | s c ore      |              | res E n g l  |
|              |              | ( 120 s c a  | ish l a n g  |
|              |              | le)          | u age p r o  |
|              |              |              | f i c i e n  |
|              |              |              | cy, s c o    |
|              |              |              | red out of a |
|              |              |              | t o tal of   |
|              |              |              | 120 p o i    |
|              |              |              | nts          |
+--------------+--------------+--------------+--------------+
| U n i v e r  | u n i ver s  | 1 to 5 w ith | R a tes u n  |
| s ity R a t  | i t y \_ r a | 5 b e ing    | i v e r s i  |
| ing          | t ing        | the h i g h  | t ies on a s |
|              |              | est r a t    | c ale f rom  |
|              |              | ing          | 1 to 5, i n  |
|              |              |              | d i c a t    |
|              |              |              | ing t h eir  |
|              |              |              | o v e r all  |
|              |              |              | q u a l ity  |
|              |              |              | and r e p u  |
|              |              |              | t a t i on.  |
+--------------+--------------+--------------+--------------+
| S t a t e m  | sop          | 1 to 5 w ith | E v a l u a  |
| ent of P u r |              | 5 b e ing    | tes the s t  |
| p ose ( S    |              | the h i g h  | r e n gth    |
| OP) S t r e  |              | est r a t    | and q u a l  |
| n gth        |              | ing          | ity of a c a |
|              |              |              | n d i d a t  |
|              |              |              | e's SOP on a |
|              |              |              | s c ale of 1 |
|              |              |              | to 5         |
+--------------+--------------+--------------+--------------+
| L e t ter of | lor          | 1 to 5 w ith | E v a l u a  |
| R e c o m m  |              | 5 b e ing    | tes the s t  |
| e n d a t    |              | the h i g h  | r e n gth    |
| ion ( L OR)  |              | est r a t    | and q u a l  |
| S t r e n    |              | ing          | ity of a c a |
| gth          |              |              | n d i d a t  |
|              |              |              | e's SOP and  |
|              |              |              | LOR on a s c |
|              |              |              | ale of 1 to  |
|              |              |              | 5            |
+--------------+--------------+--------------+--------------+
| U n d e r g  | c gpa        | 6 . 8 - 9    | R e f l e    |
| r a d u ate  |              | .92          | cts a s t u  |
| GPA          |              |              | d e n t's a  |
|              |              | ( 1 0.0 s c  | c a d e mic  |
|              |              | a le)        | p e r f o r  |
|              |              |              | m a nce in t |
|              |              |              | h eir u n d  |
|              |              |              | e r g r a d  |
|              |              |              | u ate s t u  |
|              |              |              | d i es, s c  |
|              |              |              | o red on a 1 |
|              |              |              | 0 - p o int  |
|              |              |              | s c ale      |
+--------------+--------------+--------------+--------------+
| R e s e a    | r e s e a    | 0 or 1       | I n d i c a  |
| rch E x p e  | rch          |              | tes w h e t  |
| r i e nce    |              |              | her a c a n  |
|              |              |              | d i d ate    |
|              |              |              | has r e s e  |
|              |              |              | a rch e x p  |
|              |              |              | e r i e nce  |
|              |              |              | (1) or not ( |
|              |              |              | 0).          |
+--------------+--------------+--------------+--------------+
| C h a nce of | c han c e \_ | 0 . 3 4 - 0  | R e p r e s  |
| A d mit      | o f \_ a d   | .97          | e nts the l  |
|              | mit          |              | i k e l i h  |
|              |              | (0 to 1 s c  | ood of a s t |
|              |              | a le)        | u d ent b e  |
|              |              |              | ing a d m i  |
|              |              |              | t t ed, e x  |
|              |              |              | p r e s sed  |
|              |              |              | as a d e c i |
|              |              |              | mal b e t w  |
|              |              |              | een 0 and 1  |
+--------------+--------------+--------------+--------------+

### Libraries

Load necessary packages for analysis and modeling.

```{r}
#install.packages("janitor")
#install.packages("caTools")
#Insatll required packages
#install.packages('caret')
library(caret)
library(reshape2)
library(tidyverse)
library(ggplot2)
library(readr)
library(dplyr)
library(betareg)
library(lmtest)
library(car)
library(rcompanion)
library(janitor)
library(here)
library(caTools)
library(pROC)
```

### Loading Dataset

```{r, warning=FALSE}
admission <- read_csv("adm_data.csv")
#str(admission)

admission <- admission %>%
  clean_names()

head(admission,5)
```

### Chance of Admit and TOEFL

Based on exploratory data analysis, TOEFL scores appear to be associated
with a greater chance of admission, this chance of admission is further
augmented by higher university ratings, and research experience appears
to be a strong factor in increasing both TOEFL scores and the likelihood
of admission.

```{r, warning=FALSE}
ggplot(admission, aes(x = toefl_score, y = chance_of_admit)) +
  geom_point(color = "#1f77b4", alpha = 0.6) +
  geom_smooth(method = "lm", color = "black", se = FALSE) +
  geom_smooth(method = "loess", color = "orange", se = FALSE) +
  labs(x = "TOEFL Score", y = "Chance of Admission", title = "Linear and Smooth Fit: Chance of Admission vs TOEFL Score") +
  theme_minimal() +
  theme(plot.title = element_text(color = "black", size = 14, face = "bold"),
        axis.title = element_text(size = 12),
        axis.text = element_text(size = 10),
        legend.position = "bottom")


```

The correlation coefficient of 0.791594 between TOEFL scores and the
chance of admit suggests a strong positive relationship. This indicates
that as TOEFL scores increase, the chance of admission tends to increase
as well.

```{r, warning=FALSE}
ggplot(admission, aes(x = toefl_score, y = chance_of_admit, color = as.factor(university_rating))) +
  geom_jitter(width = .2, size = I(3)) +
  scale_color_manual(values = c("#1f77b4", "#ff7f0e", "#2ca02c", "#d62728", "#9467bd")) +
  labs(x = "TOEFL Score", y = "Chance of Admission", color = "University Rating", 
       title = "Chance of Admit by TOEFL Score per University Ranking") +
  theme_minimal() +
  theme(plot.title = element_text(color = "black", size = 14, face = "bold"),
        axis.title = element_text(size = 12),
        axis.text = element_text(size = 10)) +
  geom_smooth(method = "lm", se = FALSE)


```

This trend indicates that applicants to higher-rated universities
generally have a higher chance of admission. The standard deviation
decreases as the university rating increases, suggesting more
consistency in admission chances at higher-rated universities.

```{r}
ggplot(admission, aes(x = toefl_score, y = chance_of_admit, color = as.factor(research))) +
  geom_jitter(width = .2) +
  scale_color_manual(values = c("#270181", "coral")) +
  labs(x = "TOEFL Score", y = "Chance of Admission", color = "Research Experience", 
       title = "Chance of Admit by TOEFL Score per Research Experience") +
  theme_minimal() +
  theme(plot.title = element_text(color = "black", size = 14, face = "bold"),
        legend.position = "bottom") +
  geom_smooth(method = "lm", se = FALSE)

```

Applicants with research experience (Research = 1) have a higher average
TOEFL score (approx. 110) compared to those without research experience
(Research = 0), who have an average TOEFL score of about 104.

The mean chance of admission for applicants with research experience is
significantly higher (approx. 0.796) than for those without (approx.
0.638).

This data suggests that research experience is positively associated
with both higher TOEFL scores and a greater likelihood of admission.

### Chance of Admit and GRE

These analyses suggest that higher GRE scores are strongly correlated
with an increased chance of admission. The likelihood of admission also
appears to be influenced by the university rating and is further
enhanced by research experience.

```{r}
library(ggplot2)

ggplot(admission, aes(x = gre_score, y = chance_of_admit)) +
  geom_point(color = "#1f77b4", alpha = 0.6) +
  geom_smooth(method = "lm", color = "black", se = FALSE) +
  geom_smooth(method = "loess", color = "orange", se = FALSE) +
  labs(x = "GRE Score", y = "Chance of Admission", 
       title = "Chance of Admit vs GRE Score") +
  theme_minimal() +
  theme(plot.title = element_text(color = "black", size = 14, face = "bold"),
        axis.title = element_text(size = 12),
        axis.text = element_text(size = 10),
        legend.position = "bottom") +
  guides(color = guide_legend(title = "Type of Fit", 
                              override.aes = list(linetype = c("solid", "dashed"))))


```

A correlation coefficient of 0.8026105 indicates a strong positive
relationship between GRE scores and the chance of admission. This
suggests that higher GRE scores are generally associated with a higher
likelihood of being admitted.

```{r}
ggplot(admission, aes(x = gre_score, y = chance_of_admit, color = as.factor(university_rating))) +
  geom_point() + 
  scale_color_manual(values = c("#1f77b4", "#ff7f0e", "#2ca02c", "#d62728", "#9467bd")) +
  labs(x = "GRE Score", y = "Chance of Admission", color = "University Rating", 
       title = "Chance of Admit by GRE Score per University Ranking") +
  theme_minimal() +
  theme(plot.title = element_text(color = "black", size = 14, face = "bold"),
        legend.position = "bottom") +
  geom_smooth(method = "lm", se = FALSE)

```

This trend suggests that applicants to higher-rated universities have a
higher chance of admission, with the chance of admission being most
favorable at the highest-rated universities.

```{r}
ggplot(admission, aes(x = gre_score, y = chance_of_admit, color = as.factor(research))) +
  geom_point() + 
  scale_color_manual(values = c("#270181", "coral")) +
  labs(x = "GRE Score", y = "Chance of Admission", color = "Research Experience", 
       title = "Chance of Admit by GRE Score per Research Experience") +
  theme_minimal() +
  theme(plot.title = element_text(color = "black", size = 14, face = "bold"),
        legend.position = "bottom") +
  geom_smooth(method = "lm", se = FALSE)

```

Applicants with research experience (Research = 1) have a higher average
GRE score (about 323) compared to those without research experience
(Research = 0), who have an average GRE score of approximately 309.
Similarly, the mean chance of admission is significantly higher for
applicants with research experience (approx. 0.796) than for those
without it (approx. 0.638). This indicates that research experience is
positively associated with both higher GRE scores and a greater
likelihood of admission.

### Chance of Admit and CGPA

```{r}
ggplot(admission, aes(x = cgpa, y = chance_of_admit, color = as.factor(university_rating))) +
  geom_point() + 
  scale_color_manual(values = c("#1f77b4", "#ff7f0e", "#2ca02c", "#d62728", "#9467bd")) +
  labs(x = "G.P.A Score", y = "Chance of Admission", color = "University Rating", 
       title = "Chance of Admit by G.P.A per University Ranking") +
  theme_minimal() +
  theme(plot.title = element_text(color = "black", size = 14, face = "bold"),
        legend.position = "bottom") +
  geom_smooth(method = "lm", se = FALSE)
```

Applicants with a higher g.p.a have a higher acceptance probability as
the university ranking goes from low(1) to high(5). The correlation
value of 0.87 indicates a strong positive relationship between G.P.A
score and the chance of admission. This suggests that higher a G.P.A
score are generally associated with a higher likelihood of being
admitted.

### Chance of Admission Correlation Heatmap

```{r}
library(ggplot2)
library(reshape2)
library(viridis)

# Calculate the correlation matrix
data <- cor(admission[sapply(admission, is.numeric)], use = "complete.obs")

# Reshape data for ggplot
data1 <- melt(data)

# Create the heatmap
ggplot(data1, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile() +
  scale_fill_viridis(option = "C", direction = -1) +  # Using viridis color scale
  labs(title = "Admission Correlation Heatmap", x = "", y = "") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1, size = 10),
        axis.text.y = element_text(size = 10),
        plot.title = element_text(color = "black", size = 14, face = "bold"),
        legend.position = "right") +
  geom_text(aes(label = round(value, digits = 2)), color = "white", size = 3)

  
```

The heatmap shows that GRE scores, TOEFL scores, and CGPA are strongly
and positively correlated with the chance of admission. Research
experience also positively influences admission chances, albeit to a
lesser extent than academic scores.

## Analysis and Results

Given the exploratory data analysis, we will now construct a predictive
model for the response variable chance_of_admit.

```{r}
# Scaling of Data
#admission <- scale(admission)
#admission <- as.data.frame(admission)
```

```{r}
# Splitting the Data

#make this example reproducible
set.seed(1)

#Use 70% of dataset as training set and remaining 30% as testing set
sample <- sample(c(TRUE, FALSE), nrow(admission), replace=TRUE, prob=c(0.7,0.3))
train  <- admission[sample, ]
test   <- admission[!sample, ]

#view dimensions of training set
#dim(train)

#view dimensions of test set
#dim(test)

X_train <- train

X_test = subset(test,select = -c(chance_of_admit))
keeps <- c("chance_of_admit")
y_test = test[keeps]
```

## Fitting Model 1

Model 1 = chance_of_admit \~ gre_score + toefl_score +
university_rating + sop + lor + cgpa + research + sop\*lor

```{r}
gy_logit <- betareg(chance_of_admit ~ gre_score + toefl_score + university_rating + sop + lor + cgpa + research + sop*lor, data = train)

model_summary <- summary(gy_logit)
# Extracting coefficients for the mean model
coefficients_mean <- model_summary$coefficients$mean
coeff_mean_df <- data.frame(round(coefficients_mean, 4))
colnames(coeff_mean_df) <- c("Estimate", "Std. Error", "z value", "Pr(>|z|)")

# Extract log-likelihood and pseudo R-squared
log_likelihood <- round(model_summary$loglik, 2)
pseudo_r_squared <- round(model_summary$pseudo.r.squared, 4)

# Print the formatted results
print("Coefficients (Mean Model):")
print(coeff_mean_df)
cat("\nLog-Likelihood:", log_likelihood, "\nPseudo R-squared:", pseudo_r_squared)

```

Next we use the\`regsubsets\` function to perform subset selection,
aiming to identify the most predictive variables for the
\`chance_of_admit\` outcome. This approach systematically evaluates
combinations of up to seven predictors, such as \`gre_score\`,
\`toefl_score\`, \`cgpa\`, and others, to determine their impact on the
chance of admission. The goal is to find the optimal set of variables
that best predict the admission outcome with varying model complexities.

The output indicates which variables are included in the best models at
each complexity level, revealing that \`cgpa\`, \`gre_score\`, and
\`lor\` are significant predictors in simpler models. As more variables
are added, the model becomes more complex, potentially improving
accuracy but also increasing the risk of overfitting. This analysis
helps in understanding the relative importance of different academic and
profile factors in determining admission chances. In the analyzing the
predictors and their impact, it is determined that the greatest factor
affecting the probability of admission is g.p.a. by a factor of.64. This
is followed by research with a factor of 0.15 and the combination of
statement of purpose and letter of recommendation with a factor of 0.07.

```{r}
library(leaps)
models <- regsubsets(chance_of_admit~. -serial_no, data = admission, nvmax = 9,
                     method = "seqrep")

library(broom)

model_summaries <- summary(models)

# Number of models
num_models <- nrow(model_summaries$which)

# Initialize an empty data frame for the summary
summary_table <- data.frame(Size = integer(), 
                            Variables = character(), 
                            R.Squared = numeric(), 
                            Adj.R.Squared = numeric(), 
                            BIC = numeric(), 
                            stringsAsFactors = FALSE)

# Fill the summary table
for (i in 1:num_models) {
  included_vars <- names(which(model_summaries$which[i, ]))
  vars_str <- paste(included_vars, collapse = ", ")
  summary_table <- rbind(summary_table, 
                         data.frame(Size = i, 
                                    Variables = vars_str, 
                                    R.Squared = model_summaries$rsq[i], 
                                    Adj.R.Squared = model_summaries$adjr2[i], 
                                    BIC = model_summaries$bic[i]))
}

# Print the table
print(summary_table)



```

In this analysis 10-fold cross-validation is employed to provide
assessment of the model's predictive performance by dividing the data
into ten subsets and iteratively training the model on nine subsets
while testing on the remaining one. The procedure is repeated for models
with varying numbers of predictors. Key performance metrics such as RMSE
(Root Mean Squared Error), R-squared, and MAE (Mean Absolute Error) are
calculated for each model, showing how the inclusion of additional
variables impacts the model's accuracy. The results show variations in
model performance across different numbers of predictors, with certain
models achieving lower RMSE and higher R-squared values

```{r}
# Set seed for reproducibility
set.seed(123)
# Set up repeated k-fold cross-validation
train.control <- trainControl(method = "cv", number = 10)
# Train the model
step.model <- train(chance_of_admit ~. -serial_no, data = admission,
                    method = "leapSeq", 
                    tuneGrid = data.frame(nvmax = 1:7),
                    trControl = train.control
                    )
step.model$results
```

We employ beta regression models to predict the chance of admission,
utilizing various combinations of predictors such as GRE scores, TOEFL
scores, university ratings, letters of recommendation, CGPA, and
research experience. The primary objective is to determine the optimal
set of predictors that can accurately forecast admission chances while
maintaining model simplicity.

The models, labeled \`gy_logit\` to \`gy_logit7\`, were evaluated using
the Akaike Information Criterion (AIC) and the pseudo R-squared metric.
A lower AIC value indicates a more efficient model in terms of the
trade-off between goodness of fit and complexity, while a higher pseudo
R-squared value suggests a better model fit.

The outcomes reveal that \`gy_logit4\` and \`gy_logit7\` yield the
lowest AIC values, indicating they are potentially the most efficient
models among those tested. Both these models, along with \`gy_logit\`,
also exhibit the highest pseudo R-squared values, suggesting strong
model fits.

```{r}
gy_logit <- betareg(chance_of_admit ~ gre_score + toefl_score + university_rating + lor + cgpa + research, data = train)

gy_logit1 <- betareg(chance_of_admit ~ gre_score + toefl_score + university_rating + lor + cgpa , data = train)

gy_logit2 <- betareg(chance_of_admit ~ gre_score + toefl_score + university_rating + lor + research, data = train)

gy_logit3 <- betareg(chance_of_admit ~ gre_score + toefl_score + university_rating + cgpa + research, data = train)

gy_logit4 <- betareg(chance_of_admit ~ gre_score + toefl_score + lor + cgpa + research, data = train)

gy_logit5 <- betareg(chance_of_admit ~ gre_score + university_rating + lor + cgpa + research, data = train)

gy_logit6 <- betareg(chance_of_admit ~ toefl_score + university_rating + lor + cgpa + research, data = train)

gy_logit7 <- betareg(chance_of_admit ~ gre_score + toefl_score + university_rating + lor + cgpa + research, data = train)

# Creating a data frame with AIC and pseudo R-squared values for each model
model_comparison <- data.frame(
  Model = c("gy_logit", "gy_logit1", "gy_logit2", "gy_logit3", "gy_logit4", "gy_logit5", "gy_logit6", "gy_logit7"),
  AIC = c(AIC(gy_logit), AIC(gy_logit1), AIC(gy_logit2), AIC(gy_logit3), AIC(gy_logit4), AIC(gy_logit5), AIC(gy_logit6), AIC(gy_logit7)),
  Pseudo_R_Squared = c(gy_logit$pseudo.r.squared, gy_logit1$pseudo.r.squared, gy_logit2$pseudo.r.squared, gy_logit3$pseudo.r.squared, gy_logit4$pseudo.r.squared, gy_logit5$pseudo.r.squared, gy_logit6$pseudo.r.squared, gy_logit7$pseudo.r.squared)
)


print(model_comparison)
```

### Diagnostic Measures

Upon completion of analysis of the optimal model, it is desired to
perform diagnostic analyses to inspect the goodness-of-fit of the chosen
model. This is done by assessing a measure that determines the
proportion of variance in the dependent variable that can be explained
by the independent variable and plotting graphs depicting the residuals
of the chosen model. The measure that best explains this relationship is
the $R^2$ value. The $R^2$ is a statistical measure that indicates the
percentage of the variance in the dependent variables that is explained
by the independent variables collectively.The $R^2$ formula is explained
below. $$
R^2 = 1 - \dfrac{\sum(y_{i} -\hat{y}_i)^2}{\sum(y_{i} -\bar{y})^2}
$$ where $y_{i}$ represents the observed value, $\hat{y}_i$ is predicted
values and $\bar{y}$ is the mean of the predicted values.

The values of $R^2$ are between the interval of (0,1) and the higher the
$R^2$ value, the better the independent variables explain the proportion
of variance in the dependent variable.Our analysis indicate that
gy_logit4 and gy_logit7 have the highest $R^2$ with a respective value
of (0.825 and 0.826). Both of these models are deemed the best models
but due to our desire for simplification gy_logit4 is chosen given that
it contains 4 variables vs gy_logit7 which contains 7 variables.

To further analyze the chosen model, we will perform visual diagnostics
of the residuals. Let's start by defining what are residuals. The
residuals are the difference between observed values and predicted
values.The plot below contains our predicted values,chance of admit,
which is represented by each individual data point. The prediction
observations made by the model is on the x-axis and the accuracy of the
prediction is on the y-axis. The distance from the dotted line,0,
represents how bad the prediction was for that value. Examining the
plot, we can observe that the residual bounce around the x-axis in
somewhat a random manner - they do form figures. A particular residual
does not significantly stand out from the others and the residuals band
around the 0 mean error line.These are all indicators of a good model.

```{r}
plot(gy_logit,which = 1, main = "Standardized Residuals  - Model gy_logit 4", sub.caption = "")
```

Residual plots are important because it allows us to evaluate the
prediction errors to understand whether the chosen model will provide
you with an acceptable level of accuracy.

### Conclusion

In concluding our study, the application of beta regression models has
provided valuable insights into predicting student admission
probabilities. The analysis highlights the significance of various
factors, such as GRE scores, TOEFL scores, CGPA, university ratings,
letters of recommendation, and research experience. Practically
speaking, GRE and TOEFL scores are critical as they are standard
measures of a student's academic readiness and language proficiency,
which are crucial for success in higher education settings. CGPA
reflects consistent academic performance, while university ratings may
correlate with the perceived quality and competitiveness of applicants.
Letters of recommendation and research experience offer a qualitative
assessment of a student's capabilities and potential contributions to
academic discourse.

The models that combined these predictors with the lowest AIC values and
the highest pseudo R-squared scores were deemed most effective. They
balance predictive accuracy with model simplicity, avoiding the pitfalls
of overfitting associated with more complex models. This balance is
vital for practical application, ensuring that the model remains
generalizable and relevant to various educational contexts.

Looking forward, there are several avenues for further research and
development. One potential area is the exploration of additional
variables that could impact admission chances, such as extracurricular
activities, personal statements, or socio-economic background. Another
aspect worth investigating is the application of these models in
different educational contexts and geographical locations to assess
their universality and adaptability.

Additionally, further refinement of the models could involve exploring
alternative statistical techniques or more complex machine learning
algorithms that might capture nonlinear relationships and interactions
more effectively. It would also be beneficial to consider the ethical
implications of using such predictive models in admission processes,
ensuring fairness and diversity in student selection.

In summary, this study not only contributes to the academic
understanding of factors influencing university admissions but also
offers practical implications for educational institutions and
policy-making. By leveraging statistical modeling, universities can gain
a more nuanced understanding of the admission process, aiding in the
development of more informed and equitable admission policies.

## References

Abonazel, Mohamed R., Issam Dawoud, Fuad A. Awwad, and Adewale F.
Lukman. 2022. "Dawoud--Kibria Estimator for Beta Regression Model:
Simulation and Application." Frontiers in Applied Mathematics and
Statistics 8. https://doi.org/10.3389/fams.2022.775068. Couri, Lucas,
Raydonal Ospina, Geiza da Silva, Víctor Leiva, and Jorge
Figueroa-Zúñiga. 2022. "A Study on Computational Algorithms in the
Estimation of Parameters for a Class of Beta Regression Models."
Mathematics 10 (3): 299. https://doi.org/10.3390/math10030299.
Cribari-Neto, Francisco, and Achim Zeileis. 2010. "Beta Regression in
r." Journal of Statistical Software 34 (2): 1--24.
https://doi.org/10.18637/jss.v034.i02. Douma, Jacob C., and James T.
Weedon. 2019. "Analysing Continuous Proportions in Ecology and
Evolution: A Practical Introduction to Beta and Dirichlet Regression."
Methods in Ecology and Evolution 10 (9): 1412--30.
https://doi.org/https://doi.org/10.1111/2041-210X.13234. Ferrari,
Silvia, and Francisco Cribari-Neto. 2004. "Beta Regression for Modelling
Rates and Proportions." Journal of Applied Statistics 31 (7): 799--815.
https://EconPapers.repec.org/RePEc:taf:japsta:v:31:y:2004:i:7:p:799-815.
Guolo, Annamaria, and Cristiano Varin. 2014. "Beta regression for time
series analysis of bounded data, with application to Canada Google® Flu
Trends." The Annals of Applied Statistics 8 (1): 74--88.
https://doi.org/10.1214/13-AOAS684. Ospina, Raydonal, and Silvia L. P.
Ferrari. 2012. "A General Class of Zero-or-One Inflated Beta Regression
Models." Computational Statistics & Data Analysis 56 (6): 1609--23.
https://doi.org/https://doi.org/10.1016/j.csda.2011.10.005. Patrícia L.
Espinheira, Silvia L. P. Ferrari, and Francisco Cribari-Neto. 2008. "On
Beta Regression Residuals." Journal of Applied Statistics 35 (4):
407--19. https://doi.org/10.1080/02664760701834931. Schmid, Florian AND
Maloney, Matthias AND Wickler. 2013. "Boosted Beta Regression." PLOS ONE
8 (4): 1--15. https://doi.org/10.1371/journal.pone.0061623. Simas,
Alexandre B., Wagner Barreto-Souza, and Andréa V. Rocha. 2010. "Improved
estimators for a general class of beta regression models." Computational
Statistics & Data Analysis 54 (2): 348--66.
https://ideas.repec.org/a/eee/csdana/v54y2010i2p348-366.html.
